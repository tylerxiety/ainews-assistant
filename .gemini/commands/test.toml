description = "Run Tests & Validation"
prompt = """

# Test & Validation

Prove the code works, review for issues, run automated checks. In that order.

## Part 1: Prove It Works (Behavioral)

**This is the most important part. "Build passes" is NOT verification.**

### Backend logic (parsers, algorithms, data transforms)
- Use Python REPL or curl to run the actual code path
- Pass real/representative input, check the output
- Example: `curl -X POST localhost:8000/process -d '{"url": "..."}' | jq .`

### Frontend features
- Use browser tools (Playwright) to actually click through
- Navigate to the page, perform the user action, verify result
- Check browser console for errors

### What to verify
- Happy path works as expected
- Error states handled (what if API fails? empty input?)
- Edge cases (empty list, very long input, special characters)

**Report format:**
```
Behavioral Check:
- Action: [what you did]
- Input: [what data you used]
- Expected: [what should happen]
- Actual: [what happened]
- Result: PASS/FAIL
```

## Part 2: Self-Review (Logic)

Follow `/review` criteria on touched files. Focus especially on issues static analysis can't catch:

- **Resource cleanup**: useEffect cleanup functions, event listener removal, interval clearing, file handle closing
- **Race conditions**: What if user clicks twice fast? What if two async ops complete out of order?
- **State bugs**: Can component get into invalid state? Are loading/error states handled?
- **Hardcoded values**: Strings that should be constants, values that should be config

See `/review` for full checklist (security, async patterns, TypeScript, etc.).

## Part 3: Automated Checks

Run these, report PASS/FAIL:

1. **Backend types**: `cd backend && uv run mypy . --ignore-missing-imports`
2. **Frontend types**: `cd frontend && npx tsc --noEmit`
3. **Backend lint**: `cd backend && uv run ruff check .` (if configured)
4. **Frontend lint**: `cd frontend && npm run lint` (if configured)
5. **Frontend build**: `cd frontend && npm run build`

## Output Format

```
## Test Results

### Behavioral Checks
- [Feature/function tested]
  - Action: [what was done]
  - Result: PASS/FAIL — [observed behavior]

### Self-Review
- Resource cleanup: PASS/FAIL — [notes]
- Race conditions: PASS/FAIL — [notes]
- State handling: PASS/FAIL — [notes]

### Automated Checks
- Backend types: PASS/FAIL
- Frontend types: PASS/FAIL
- Build: PASS/FAIL

### Issues Found
- [File:line] Description

### Summary
Behavior verified: YES/NO
Issues to fix: X
```

## Scope

If testing specific files/features (e.g., from `/verify`), focus on those areas only.

## Per AGENTS.md

- Do NOT create standalone test scripts (`test_*.py`, `check_*.py`)
- Use REPL, curl, or browser tools instead
- Add temporary logging if needed, then remove it
"""
