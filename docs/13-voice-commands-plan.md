# Voice Command & Q&A via Gemini Live API

**Overall Progress:** `85%`

## TLDR

Add a tap-to-enable voice mode to the PWA. While the newsletter plays, users can speak commands (play, pause, next, previous, bookmark, rewind, forward) or ask Q&A questions â€” all hands-free. Playback pauses on speech, resumes after handling. Uses Gemini Live API with function calling through a backend WebSocket proxy (iOS Safari compatibility). Client-side VAD for instant speech detection. AudioContext for mic capture (PCM) and streaming Q&A audio playback. Newsletter stays on `<audio>`; Q&A audio plays via AudioContext.

## Critical Decisions

- **Server-side proxy**: Browser â†’ FastAPI WebSocket â†’ Gemini Live (avoids iOS 26 Safari WebSocket upgrade bug with GCP endpoints)
- **Client-side VAD (Silero via @ricky0123/vad-web)**: Pause newsletter instantly on speech start; Gemini Live has no `speech_started` event
- **Function calling for commands**: Define play/pause/next/etc. as Gemini tools â€” no custom classifier needed. Anything not a command gets a Q&A audio response
- **AudioContext for all voice I/O**: Mic capture as 16kHz PCM (Gemini input format), Q&A playback as 24kHz PCM (Gemini output format). `<audio>` element stays for newsletter MP3s only
- **AUDIO-only responses + transcripts**: Session uses `response_modalities: ["AUDIO"]`. Use real-time audio transcripts from the API as the answer text in the Q&A panel; show question text as `[user question]`
- **No wake word**: Tap-to-enable toggle is the activation gesture
- **Silent visual feedback for commands**: No spoken confirmation â€” newsletter resumes immediately
- **One Gemini Live session per issue**: Newsletter context in system prompt at session start. Handle connection reset (~10 min) and 15-min session limit with resumption tokens
- **Keep existing Q&A flow as fallback**: If voice mode WebSocket fails, existing mic-button Q&A still works
- **Vertex AI backend**: Use `google-genai` SDK configured for Vertex AI (ADC, project, location `us-central1`)

## Context for Implementer

- **Existing Q&A flow**: `frontend/src/components/Player.tsx` swaps the `<audio>` element to play Q&A audio and restores newsletter state. Voice mode must not break this fallback path.
- **Mic recording hook**: `frontend/src/hooks/useAudioRecorder.ts` handles manual Q&A (MediaRecorder); voice mode should disable/replace mic button while active.
- **Config source of truth**: All settings live in `config.yaml` (frontend consumes via `frontend/src/config.ts`, backend via `backend/config.py`).
- **Backend deps**: Python deps are managed in `backend/pyproject.toml` with `uv` (no `requirements.txt` updates).
- **iOS constraints**: AudioContext must be created on user tap; keep newsletter playback on `<audio>` element.

## Tasks

- [x] ğŸŸ© **Step 1: Backend â€” Gemini Live WebSocket proxy**
  - [x] ğŸŸ© Add `google-genai` to `backend/pyproject.toml` dependencies (uv-managed)
  - [x] ğŸŸ© Create `backend/voice_session.py` â€” manages one Gemini Live session per WebSocket connection
    - Connect to Gemini Live (`gemini-live-2.5-flash-native-audio`) via `google-genai` SDK configured for Vertex AI (`project`, `location`)
    - Configure session: system prompt with newsletter context, response modalities `["AUDIO"]`
    - Define function tools: `play`, `pause`, `next_segment`, `previous_segment`, `bookmark`, `rewind(seconds=5)`, `forward(seconds=5)`
    - Forward incoming PCM audio chunks from client â†’ Gemini Live session
    - Receive Gemini responses: forward tool calls as JSON, forward audio chunks as binary, forward answer transcripts as JSON
    - Handle connection reset (~10 min) + session expiry (~15 min) with resumption tokens
  - [x] ğŸŸ© Add WebSocket endpoint `ws /ws/voice/{issue_id}` in `main.py`
    - On connect: fetch newsletter context from Supabase, open Gemini Live session
    - Bidirectional relay: client audio â†’ Gemini, Gemini responses â†’ client
    - On disconnect: close Gemini Live session, cleanup
  - [x] ğŸŸ© Add `voiceMode` config section to `config.yaml` (model name, region, session timeout, VAD sensitivity, resume delay)

- [x] ğŸŸ© **Step 2: Frontend â€” AudioContext mic capture + PCM streaming**
  - [x] ğŸŸ© Create `frontend/src/hooks/useVoiceMode.ts` â€” core voice mode hook
    - Open WebSocket to `/ws/voice/{issueId}`
    - Create `AudioContext` (on user tap gesture)
    - Capture mic via `getUserMedia` â†’ `AudioWorkletNode` â†’ downsample to 16kHz mono PCM
    - Stream PCM chunks over WebSocket as binary messages
    - Receive WebSocket messages: parse JSON (tool calls) vs binary (audio response chunks)
    - Expose state: `isVoiceModeActive`, `isListening`, `isSpeaking` (from VAD), `lastCommand`
    - Handle reconnection on session expiry
  - [x] ğŸŸ© Create `frontend/src/worklets/pcm-capture.worklet.ts` â€” AudioWorklet processor
    - Capture mic audio, resample to 16kHz if needed, output Int16 PCM chunks
  - [x] ğŸŸ© Add `@ricky0123/vad-web` dependency to `package.json`

- [x] ğŸŸ© **Step 3: Frontend â€” Client-side VAD integration**
  - [x] ğŸŸ© Integrate Silero VAD in `useVoiceMode.ts`
    - On `onSpeechStart`: pause `<audio>` newsletter immediately, signal "user speaking" state
    - On `onSpeechEnd`: if Gemini returns a tool call â†’ resume newsletter after command execution; if Q&A audio response â†’ resume after playback ends
    - Configure sensitivity thresholds (from config)

- [x] ğŸŸ© **Step 4: Frontend â€” Q&A audio playback via AudioContext**
  - [x] ğŸŸ© Add playback logic in `useVoiceMode.ts`
    - Receive 24kHz PCM chunks from WebSocket
    - Queue and play via `AudioContext` using `AudioBufferSourceNode` (streaming: schedule sequential buffers)
    - Track playback completion â€” trigger newsletter resume after last chunk plays
  - [x] ğŸŸ© Handle AudioContext suspension/resumption on iOS (re-resume on user interaction if suspended)

- [x] ğŸŸ© **Step 5: Frontend â€” Command execution bridge**
  - [x] ğŸŸ© Wire tool call JSON from WebSocket to Player.tsx actions
    - `play` â†’ `handlePlay()`
    - `pause` â†’ `handlePause()`
    - `next_segment` â†’ advance to next segment (reuse existing `handleEnded` auto-advance logic)
    - `previous_segment` â†’ go to previous segment
    - `bookmark` â†’ trigger bookmark on current segment (reuse existing bookmark handler)
    - `rewind(seconds)` â†’ `audioRef.current.currentTime -= seconds`
    - `forward(seconds)` â†’ `audioRef.current.currentTime += seconds`
  - [x] ğŸŸ© Add visual feedback for commands â€” brief toast or icon flash in AudioBar (e.g., "Bookmarked" toast, rewind icon animation)

- [x] ğŸŸ© **Step 6: Frontend â€” Voice mode UI**
  - [x] ğŸŸ© Add voice mode toggle button to AudioBar (replace or augment existing mic button)
    - Inactive: mic icon (current)
    - Active: pulsing/glowing mic icon + "Listening" indicator
    - Tap toggles voice mode on/off
  - [x] ğŸŸ© Update SidePanel Q&A tab to show voice mode state
    - Show real-time status: "Listening...", "Processing...", command executed feedback
    - Q&A messages still appear in conversation history (from Gemini Live audio transcripts)
    - Question text is shown as `[user question]` for voice mode
  - [x] ğŸŸ© Add voice mode active indicator in AudioBar (subtle persistent indicator when mode is on)

- [x] ğŸŸ© **Step 7: Config and integration**
  - [x] ğŸŸ© Add voice mode config to `config.yaml` and `frontend/src/config.ts`
    - `voiceMode.model` = `gemini-live-2.5-flash-native-audio`
    - `voiceMode.region` = `us-central1`
    - `voiceMode.sessionTimeoutMs`, `voiceMode.vadSensitivity`
    - `voiceMode.resumeDelayMs` (delay before resuming newsletter after Q&A)
  - [x] ğŸŸ© Update Player.tsx to integrate `useVoiceMode` hook
    - Pass command handlers to the hook
    - Coordinate voice mode state with existing Q&A state (voice mode replaces manual Q&A when active)
    - Handle edge cases: voice mode + manual mic button, voice mode during segment transitions

## Testing

**Test Notes (2026-01-28):** Playwright on ngrok failed to load `silero_vad.onnx` (protobuf parsing error), so voice mode could not start. Remaining scenarios blocked.

### Approach
Test on iOS Safari PWA + Chrome desktop using browswer agent. Backend WebSocket tested via Python WebSocket client script.

### Test Scenarios
- [ ] ğŸŸ¥ Tap voice mode toggle â†’ WebSocket connects, mic access granted, "Listening" indicator shown
- [ ] ğŸŸ¥ Say "pause" while newsletter plays â†’ newsletter pauses within 200ms of speech start (VAD), Gemini returns `pause` tool call, visual feedback shown
- [ ] ğŸŸ¥ Say "play" while paused â†’ newsletter resumes from saved position
- [ ] ğŸŸ¥ Say "next" â†’ advances to next segment, plays from start
- [ ] ğŸŸ¥ Say "bookmark" â†’ current segment bookmarked, toast shown, newsletter resumes
- [ ] ğŸŸ¥ Say "rewind" â†’ currentTime decreases by 5 seconds, newsletter resumes
- [ ] ğŸŸ¥ Ask a question (e.g., "What did they say about React?") â†’ newsletter pauses, Gemini responds with audio, audio plays through AudioContext, newsletter resumes after answer
- [ ] ğŸŸ¥ Tap voice mode toggle off â†’ WebSocket closes, mic released, indicator removed
- [ ] ğŸŸ¥ Session expires (15 min) â†’ auto-reconnects with resumption token, no user-visible interruption
- [ ] ğŸŸ¥ iOS Safari PWA: voice mode works with screen on, AudioContext not suspended during active use
- [ ] ğŸŸ¥ Voice mode off â†’ existing mic-button Q&A flow still works unchanged

### Acceptance Criteria
- [ ] Voice commands execute and newsletter resumes within 500ms of speech end (excluding network latency)
- [ ] Newsletter audio pauses within 200ms of user starting to speak (client-side VAD)
- [ ] Q&A audio response streams and plays without audible gaps or artifacts
- [ ] Newsletter resumes from exact saved position after command or Q&A
- [ ] Voice mode toggle is a single tap â€” no multi-step activation
- [ ] Existing non-voice Q&A flow (mic button â†’ record â†’ stop â†’ answer) works when voice mode is off
- [ ] No microphone access requested until user taps voice mode toggle
- [ ] Works on iOS Safari PWA in foreground with screen on

## Implementation Notes (Deviations)

- **Command handling**: Added server-side command detection from input audio transcription and suppress model output for those turns, instead of relying solely on Gemini tool calls. Reason: model occasionally answered with Q&A (â€œlet me checkâ€) for command words like â€œnext,â€ so commands were dropped.
- **Tool set**: Removed `next_segment`/`previous_segment` tool declarations and standardized on `next`/`previous` only. Reason: user requirement that only play/pause/next/previous/bookmark/rewind/forward are tools.
- **State handling**: Voice command segment navigation now uses refs for current indices to avoid stale React state when multiple commands are issued quickly. Reason: repeated â€œnext/previousâ€ commands were ignored after the first due to stale closures.
- **Prompt config**: Added `prompts.voiceMode` in `config.yaml` and use it for the voice mode system prompt. Reason: make prompt adjustable without code changes and tighten command-only tool usage.
- **UI behavior**: Voice mode no longer auto-opens the left panel (Q&A tab) when speaking/answering. Reason: keep voice mode hands-free and avoid unexpected UI changes.
